{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dd5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "SLURM_PATH = '/home/yandex/MLWG2025/amitr5'\n",
    "CACHE_DIR = f'{SLURM_PATH}/tmp/hf_cache'  # Changed to /tmp to avoid quota issues\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "if SLURM_PATH in os.getcwd():\n",
    "    os.environ[\"PIP_PATH\"] = f\"{SLURM_PATH}/BaryGNN/anaconda3/envs/conf/bin/pip\"\n",
    "    os.environ[\"TEMP_DIR\"] = CACHE_DIR\n",
    "    os.environ[\"HF_HOME\"] = CACHE_DIR\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n",
    "    os.environ[\"HF_DATASETS_CACHE\"] = CACHE_DIR\n",
    "    os.environ[\"HF_HUB_CACHE\"] = CACHE_DIR\n",
    "    os.environ[\"TMPDIR\"] = CACHE_DIR\n",
    "    # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b564846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "def reload(*module_names):\n",
    "    for name in module_names:\n",
    "        if name in sys.modules:\n",
    "             importlib.reload(sys.modules[name])\n",
    "             print(f\"{name}- reloaded\")\n",
    "        else:\n",
    "            print(f\"{name}- NOT FOUND!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e715695e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6319076043490789a3949086718e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1383c2efcafa42ffa000f6a395c9ea02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: ['train', 'test_domain', 'test_task', 'test_website']\n",
      "Number of samples in test_domain: 4060\n",
      "Number of samples in test_task: 1339\n",
      "Number of samples in test_website: 1019\n",
      "Total number of test samples: 6418\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('./Mind2Web/src') \n",
    "\n",
    "ds = load_dataset(\"osunlp/Multimodal-Mind2Web\", cache_dir=CACHE_DIR)\n",
    "\n",
    "print(\"Dataset splits:\", list(ds.keys()))\n",
    "# Access the test splits\n",
    "test_domain_ds = ds['test_domain']\n",
    "test_task_ds = ds['test_task']\n",
    "test_website_ds = ds['test_website']\n",
    "\n",
    "print(f\"Number of samples in test_domain: {len(test_domain_ds)}\")\n",
    "print(f\"Number of samples in test_task: {len(test_task_ds)}\")\n",
    "print(f\"Number of samples in test_website: {len(test_website_ds)}\")\n",
    "print(\"Total number of test samples:\", len(test_domain_ds) + len(test_task_ds) + len(test_website_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea594029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_prediction.dataloader- reloaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c40a724b9cb43d792495f1345e9f89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ea0166ea42428ea70c153fa5d1a79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 2048)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5Tokenizer\n",
    "from typing import Dict, Any\n",
    "import sys\n",
    "sys.path.append('./Mind2Web/src') \n",
    "from action_prediction.metric import ActionEvaluatorGeneration, ActionEvaluatorMultiChoice\n",
    "\n",
    "reload('action_prediction.dataloader')\n",
    "from mind2web.dataloader import MultiChoiceDataset, get_data_split, subsample_by_annotation\n",
    "\n",
    "\n",
    "def tensorize_item(item: Dict[str, Any], device: str):\n",
    "    \"\"\"\n",
    "    Convert the model_input dict returned by MultiChoiceDataset.__getitem__\n",
    "    (lists of ints) into tensors appropriate for model.generate.\n",
    "    \"\"\"\n",
    "    input_ids = torch.LongTensor(item[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.LongTensor(item[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, cache_dir=CACHE_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\", cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "dataset = MultiChoiceDataset(\n",
    "        flattened,\n",
    "        tokenizer,\n",
    "        num_candidates=5,\n",
    "        max_context_len=512,\n",
    "        # mode=\"generation\",  # use generation formatting\n",
    "        mode=\"multichoice\",  # use multichoice formatting\n",
    "    )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"osunlp/MindAct_ActionPrediction_flan-t5-xl\" #\"Qwen/Qwen-3.5-VL-Base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f8bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mind2web.dataloader- reloaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac3f339071a4111a6d437a55fe877fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (125915272 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6058f2b2cc1c4ef19cc073e365a05117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload('mind2web.dataloader')\n",
    "from mind2web.dataloader import MultiChoiceDataset, get_data_split, subsample_by_annotation\n",
    "from mind2web.dataloader import subsample_by_annotation\n",
    "SEED = 42\n",
    "\n",
    "# Construct MultiChoiceDataset like evaluate.py does.\n",
    "\n",
    "split_file = \"test_task\"  # or \"test_task\" or \"test_website\"\n",
    "\n",
    "candidate_results = pd.read_pickle(f\"candidates/scores_{split_file}.pkl\")\n",
    "flattened = get_data_split(\n",
    "    data_dir=\"osunlp/Multimodal-Mind2Web\",\n",
    "    split_file=split_file,\n",
    "    candidate_results=candidate_results,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "cal_set, test_set = subsample_by_annotation(flattened, frac=0.05, seed=SEED)\n",
    "cal_ds = MultiChoiceDataset(cal_set, tokenizer, num_candidates=5, max_context_len=512)\n",
    "test_ds = MultiChoiceDataset(test_set, tokenizer, num_candidates=5, max_context_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad6a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c81be44a64942d0897f48d171eea223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa0bfca02c546739338f9df205b2151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621b565636f24e0da49c67504d38fc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f2556ce95a4bc6bd4a9f1bd36ed32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c523be35f1dc4311b78f11aefed1e4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f6b1b53293494d804a9bc89120a360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5b849b0ffe474db4c72c0e1d743a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f1e8a1c97c4b97b8ba57b3c23ec129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decaa843255440969a4414c68524ff7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mind2web.dataloader import MultiChoiceDataset, get_data_split, subsample_by_annotation\n",
    "\n",
    "def build_split_datasets(\n",
    "    split_files,\n",
    "    tokenizer,\n",
    "    seed=42,\n",
    "    frac=0.1,\n",
    "    num_candidates=5,\n",
    "    max_context_len=512,\n",
    "    data_dir=\"osunlp/Multimodal-Mind2Web\",\n",
    "    cache_dir=None,\n",
    "    candidates_dir=\"candidates\",\n",
    "):\n",
    "    \"\"\"Return (cal_dict, test_dict) keyed by split name.\"\"\"\n",
    "    cal_dict, test_dict = {}, {}\n",
    "    for split_file in split_files:\n",
    "        cand_path = f\"{candidates_dir}/scores_{split_file}.pkl\"\n",
    "        candidate_results = pd.read_pickle(cand_path)\n",
    "\n",
    "        flattened = get_data_split(\n",
    "            data_dir=data_dir,\n",
    "            split_file=split_file,\n",
    "            candidate_results=candidate_results,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        cal_set, test_set = subsample_by_annotation(flattened, frac=frac, seed=seed)\n",
    "        cal_dict[split_file] = MultiChoiceDataset(\n",
    "            cal_set, tokenizer, num_candidates=num_candidates, max_context_len=max_context_len\n",
    "        )\n",
    "        test_dict[split_file] = MultiChoiceDataset(\n",
    "            test_set, tokenizer, num_candidates=num_candidates, max_context_len=max_context_len\n",
    "        )\n",
    "    return cal_dict, test_dict\n",
    "\n",
    "\n",
    "cal_dict, test_dict = build_split_datasets([\"test_domain\", \"test_task\", \"test_website\"],\n",
    "                                            tokenizer,\n",
    "                                              seed=42,\n",
    "                                              frac=0.1,\n",
    "                                              cache_dir=CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25c57303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating calibration set: 100%|██████████| 37/37 [03:18<00:00,  5.36s/it]\n",
      "Generating calibration set: 100%|██████████| 37/37 [03:18<00:00,  5.36s/it]\n",
      "Generating calibration set: 100%|██████████| 11/11 [00:51<00:00,  4.65s/it]\n",
      "Generating calibration set: 100%|██████████| 11/11 [00:51<00:00,  4.65s/it]\n",
      "Generating calibration set: 100%|██████████| 4/4 [00:18<00:00,  4.61s/it]\n",
      "Generating calibration set: 100%|██████████| 4/4 [00:18<00:00,  4.61s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "choices_to_token_ids = dataset.choices_token_ids_mapping()\n",
    "max_new_tokens = 1 #50\n",
    "\n",
    "\n",
    "def run_calibration(cal_sets:list):\n",
    "    outputs = []\n",
    "    for cal_ds in cal_sets:\n",
    "        for i, item in enumerate(tqdm(cal_ds, desc=\"Generating calibration set\"), 1):\n",
    "                model_input = tensorize_item(item, device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out = model.generate(\n",
    "                        **model_input,\n",
    "                        eos_token_id=model.config.eos_token_id,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True,\n",
    "                    )\n",
    "\n",
    "                decoded = tokenizer.batch_decode(out[\"sequences\"], skip_special_tokens=True)[0]\n",
    "                labels_tokens = item.get(\"labels\")\n",
    "            \n",
    "                # Calculate choice probabilities\n",
    "                logits = out[\"scores\"][0][0]\n",
    "                all_probs = F.softmax(logits, dim=-1)\n",
    "                probs = all_probs[list(choices_to_token_ids.values())]\n",
    "                choices_probs = dict(zip(choices_to_token_ids.keys(), probs.cpu().tolist()))\n",
    "                labels = item.get(\"labels\").strip()\n",
    "                outputs.append(\n",
    "                    [\n",
    "                        i, \n",
    "                        dataset.data[i].get(\"annotation_id\"),\n",
    "                        dataset.data[i].get(\"action_uid\"),\n",
    "                        decoded,\n",
    "                        labels.split('.')[0],\n",
    "                        labels, \n",
    "                        choices_probs,\n",
    "                        choices_probs.get(decoded, 0)\n",
    "                    ]    \n",
    "                )\n",
    "        \n",
    "\n",
    "    cols = [\"index\", \"annotation_id\", \"action_uid\", \"generated\", \"label\",'label_text', \"choices_probs\", \"prob\"]\n",
    "    results_df = pd.DataFrame(outputs, columns=cols)\n",
    "    return results_df\n",
    "\n",
    "cal_df = run_calibration(list(cal_dict.values()))\n",
    "if len(cal_df):\n",
    "    cal_df.to_pickle(\"cal_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf2d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<mind2web.dataloader.MultiChoiceDataset at 0x7c29bd123d90>,\n",
       " <mind2web.dataloader.MultiChoiceDataset at 0x7c29bd036c40>,\n",
       " <mind2web.dataloader.MultiChoiceDataset at 0x7c2d144bf790>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fdf2a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005385100841522217\n"
     ]
    }
   ],
   "source": [
    "results_df['correct'] = results_df['generated'] == results_df['label']\n",
    "grouped = results_df.groupby('annotation_id')\n",
    "score_per_group = 1 - grouped['prob'].min()\n",
    "alpha = 0.05\n",
    "\n",
    "N = len(score_per_group)\n",
    "k = int(np.ceil((N + 1) * alpha))\n",
    "sorted_scores = np.sort(score_per_group.values)\n",
    "threshold = sorted_scores[k - 1]\n",
    "print(threshold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbf2d208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAADvCAYAAACeyYwKAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAOPlJREFUeJzt3QmcTXX/B/Dv2MbY9zXLyE7WUviLQkiiiFBIlooia57K+oSSpaeUyhNRduIJ2cla2Stb1uw7YYyxnf/r89W53Xvn3nvunbkzc5fP+/W6zJw595zzO+v3/NYIwzAMISIiIvIglac/EhERETFgICIiIq8wh4GIiIgsMWAgIiIiSwwYiIiIyBIDBiIiIrLEgIGIiIgsMWAgIiIiSwwYiIiIKPQDhqJFi0qHDh1SejNC3qhRo6RYsWKSOnVqqVSpUkpvTlA7c+aMtGjRQnLmzCkREREybtw4CQRr1qzR7cH/FHz7dPDgwbqu8+fPS7A4cuSIbvPkyZPjpcMefu/evbuktKJh/rwJqIABJw1OjC1btrj8e506daR8+fKJXs/ixYv1pCTvLFu2TPr16yc1a9aUSZMmyfDhw7nrEuHNN9+UpUuXyoABA2Tq1KnSsGHDgN2f06ZNC5iAhu7B9Td//nzujiSyceNGfT5cvnw5JPfxxsSkzwggkyZNwrgWxubNm13+vXbt2ka5cuUcpt24ccO4efOmT+vp1q2broe8079/fyNVqlRGXFwcd5kf5M2b12jbtm3A7cs7d+4YsbGx+r+pcePGRpEiRVJ0u4KZq32aWBkzZjTat28fb/qgQYP0vnbu3DkjWBw+fFi3Gfd+061bt3Sf2cM8uG8nh1GjRun6sG3ObiTgeRNoPKXPSkDlMCREZGSkpE2bVoJJTEyMBJOzZ89KVFSUpEuXLqU3JSRgf2bLls1vy7tx44bcvXs30ctJlSqVpE+fXv8Pdbdv35abN28m+XrCaZ/6S5o0aXSfBeL9NjIInzf+FPRnsXOZ0q1bt2TIkCFSokQJPelQTvx///d/snz5cv075h0/frz+jOIP82N/cvXu3VsKFSqkJ0epUqXkww8/RHaEw3pjY2PljTfekFy5cknmzJnl6aeflhMnTuiy7Is7zPK43bt3S5s2bSR79uy6PfDrr7/q9qBuALY1X7580rFjR7lw4YLDusxl/PHHH/LCCy9I1qxZJXfu3PLuu+/qdh07dkyaNm0qWbJk0WWMHj3a65vmsGHD5P7779e0Yl/+61//kri4ONs8WC+KIbBfzH1lX97ortgI6X3sscckQ4YMUrBgQfnggw9cPjhffvllyZs3r6a/YsWK8vXXX7ss48Qx+OKLL2zb+tBDD8nmzZvFWz///LM8+eSTuv8zZswoFSpUkI8++shhnlWrVkmtWrX073igY5/u2bPHb8fCLHLDfDgHnc+9Q4cOyXPPPSc5cuTQ/fbII4/IokWLXJaJz5gxQ9555x3dt5j3ypUrei5lypRJjh49Kk899ZT+jL+b5/tvv/0mjz/+uKavSJEiWtzgqbwdxxLr//PPP23binPk2rVruowePXrE28/Hjx/Xei4jRozweDyw/VWrVtVrB/vqgQceiHc8kGWK4husE8f8vvvuk3bt2jmU0ft6DqF4xTyHcI7C3r17tU4J9juW8eCDD8r//vc/h2VY3Vd8qcPgyzXiDMvCtYg0msfEuUwd+w3TcA7j/HzppZfk+vXr8Zb1zTff6DHAywDS/vzzz+v56w3c67DfCxQooPsyOjpaXn31VVsQdvHiRenTp48eV5yHOMaNGjWSnTt3Wi7bVR0G07fffqv3ZBwDbPvatWtdfjeh91t8v2/fvvoz0mTuY5xD7uow+HLdzpo1S9577z09l7ENdevWlQMHDoiVq1evSs+ePW3XQp48eaR+/fqybdu2ePc5FHHiuGNbateuLRs2bPA6fVbSSAD666+/XFbcwUVrBTsEN6tOnTpJtWrV9EaKOhHYsdjBXbt2lZMnT+qFjvJje7iR48G/evVqvRhQuQ9lzdjBuEDGjh1rmxcnDQ7+iy++qCfIjz/+KI0bN3a7XTihcLNB+aMZfGAbcLLhgsbJu2vXLn0o4v+ffvop3kXTqlUrKVOmjIwcOVJPyH//+996kn7++ef6IHj//ff1gsKFigfqo48+6nFfYR/hxoObJYIknGzYd3hIfvfddzoP9hG26ZdffpGJEyfqtBo1anhc7qVLl/SkffbZZ6Vly5YyZ84c6d+/v948cNMwAy7cOHGxoDITTt7Zs2frfsUNz/lhhIcbLhocP+wX3FyxfOw/q4gf+xkP0Pz58+tysa+RxoULF9rWs2LFCt023ExwDmH7Pv74Y623gXMHF2pijwU+2J84Z3Au4uFnXxES+xU3dgSieCDh2OB8xP575plnHNaPQA85Plg+Ajwz9+fOnTuaDqwL+wjbgP2LB/zbb78tbdu21f02YcIEXX/16tV137uC+XEtIggwz33c/PHB9sycOVPGjBmjAYJp+vTpen5jPZ6OR+vWrfVmif0EOB64sZnHA0EJgjdMx029SpUqek/Agxzbg0Dd13MIgS9yY7p06aI3XRwvXGs4xnhgv/XWW7qfcF03a9ZM5s6da9vvVvcVX3lzjbiC88fcBqQDEADZw/KwL7C92D5ct3jAmPsa8NBCkIt5sbxz587p+Y7zZvv27R5zwHD/xPqxj7ENpUuX1vsj0oDzF+cirkvUs8B9D9uC8xvXBh5geJgj0PAV7rE453B94Ph9+umnug9xb3Ku25bQ+y2OB14GcB7jnMd5BngpcMXX6xb3C+Q24brFtYVrFNcK7r2evPLKK7o8nOdly5bVIGf9+vV6feDaMF94cO4gkBo0aJCuB+c87kfr1q3TY+Zr+uIxArAOg6ePcx0GlK/al+dVrFhRy10TUodh/vz5Ov3f//63w/QWLVoYERERxoEDB/T3rVu36nw9e/Z0mK9Dhw46HWWJzuWKrVu3jre+69evx5s2ffp0nX/t2rXxltGlSxfbtNu3bxv33XefbtfIkSNt0y9dumRERUW5LOO0t2PHDl1mp06dHKb36dNHp69atco2DctCuak3UM8E358yZYptGuo+5MuXz2jevLlt2rhx43S+b775xjYNZYPVq1c3MmXKZFy5csWhjDNnzpzGxYsXbfMuWLBAp3///fcetwf7KTo6Ws8T7Bt7d+/etf1cqVIlI0+ePMaFCxds03bu3Kl1N9q1a+fXY+GqPBbnEqavW7fONu3q1au67UWLFrWVga9evVrnK1asWLzzB+vB34YPHx5vG7BtM2bMsE3fu3dvvHPVXDb+t6rDsHTpUp33hx9+cJheoUIFPQc86dGjh5ElSxbdb+4MHDhQlz9v3rx4fzOPm6/nENZ59uxZh2XVrVvXeOCBB7Rs2n75NWrUMEqUKOHTfcUVV/vU22skoXUYOnbs6DD9mWee0evHdOTIESN16tTGe++95zDfb7/9ZqRJkybedGe4HnBduKprZh4b7E/nehs4DpGRkcbQoUM91mEw02HPvP9v2bLFNu3PP/800qdPr+nz5/3WUxl/Eafnja/XbZkyZRzqgn300Uc6Hfvek6xZs3qsw4H9jvO1QYMGDvc1pBnbUr9+fa/SZyUgiySQhYpo0PmDbGQriIwRMe7fvz9BrSfwtoRI0R7evnHO/vDDD/r7kiVL9P/XXnvNYb7XX3/dY4ToDFmBJrz54A0KuRXgnNUEeBMwYTuRdYrtQm6IffqRZYdI2iqt0KtXr3hpBecsNV/gDRTZ9Sa8cSC6td8mrB9RPt40TcgpwL7H2yXeJpzf6JG9aMLbJ1ilE29Lhw8f1uw857cmMwfn1KlTsmPHDn0zxVunCecb3h7NfZVUx8LcH9hHZvapuR/xBofsQjP73NS+fXuH88fdtpnbgDdnvE2aMA1/82bbXKlXr56+JSIHw/T7779rtq/9sXcF60W2uqfsfLzdo3jB+Q3N/rj5eg41b97c4U0K2eZ4K8N+Qe4Vrj988PbWoEEDvYfgzTmx95WEXiMJ5XyvwbWCNCFXBObNm6d1XpBuM834YF/irRw5rO7ge8g5aNKkiZ7z7o4NcgDMehvI9cL6kWacd67ubd5Abhjenk2FCxfW4j/kAmMdnvZBQu633ljs43WL3A37umDe3sdw/iEXArk7ruD+hXMTxTDY1+YxxXWGnDwU3filnpMEIBwA3JCcP/YPDHeGDh2qWWUlS5bU7D0UJ+Am5g2U1eImiHJVe8h6Nv9u/o+LwTkrt3jx4m6X7SrbFzcsZJui/BUnM25m5nzIrnKGC8QeyqlQDmZmK9lPR5anVVqRBudtxk0DJ6eZ1oRA+ZxzcQqOnf02Yfm4OTlXBnPe1+7Sbp4L5jKRPX369GmHDxw8eFD/99Qc11wXbmbOsD3mhZdUx8LcBnfrt99Gk7tiBGyDc/YitsHVMfF221zBcUNWKh4eZvk4ggesH9nBniDQxvWJ7FNsF4oczCDchONm1YTa13PIeZ+hKANBHrLmsc/sP8jSNetIJPa+ktBrJKGsrhU8WJBu7DvndCOL20yzKyi6QOBhdWzwcEKWN9aB4AHXBZaPfebq3uYNLMsZjgfOP2yXv++33vjTx+vW6ti4g6ILBOSoW4fnI4rI7IMMM5DFi4TzMUWRFIotE5rGgK/DkBgog8PNZsGCBdp/AHYWTlyU2dq/eSU3V2+DiPDRJhY3H9SXQGSKCw3lcq6iQfuyYk/TwLmSpjvuKhclRmK3KSHLRNkmond/rS8h25MU6XbHXe6Cu21Iim1DHQh06IWgAW/5qGeCuiIIRDxBeTreiPBmiFw7fFDWiuU5V1hMyn1mXmMoT0aOgitmQO3v+0pSnitWy0a6cd1jv7uaF/ehxELdAQRiCAZR3wY5dwjskNPnjzfd5LjfJoXUCTzu2HbkRqBuGc4/XHeok4LcIgTe5vZjuruO9fxxXEMuYACcnHh44IOsSVzsiMjMC9vdQxI1x1H5DdmT9rkMqEVt/t38HwcIWd32Ua83tV1NiChXrlypNa8HDhxom+6vLE8rZhqwPjMaNivx4E3KTGtSrh9vG9gG+zdE533tLdzwXWVxmxXCEJ0jl8rdtsC+ffvi/Q3bg7cjZOknJWyDu/Xbb2Ny8xRQ4i2zcuXKmrOAN2a0zkDFOW8gWxbZ2vjgHECuAyrF4SGDhzSOG45ZUp5DqOBqFmO4Ozd8ua8kl8QG+di3eEDh7Rpv6L7AGytaPFgdG1TQQwuQ//73vw7TcW9xzoXzlqt7IyrwoTWAVaU9X+63vuzfIsl43aLSNq4TfJALhMqOqLyKgMG8z+HYWJ3LiTl/ArJIIjGcmyQiqsINyL6poHnzd+7pCs3uUBb2ySefOEzHmwR2sll72XwbQS1de97eLO0jTefIMrl61UNaXa0Ptd7BU4sPf60fxQbIGbBv5ol9iGOG2tS+XkzORViAiwo3RqTT+Xib+x7fRVSOt1v7eXBTRDRv7qukhHWgtvemTZts01AMglrcaKGBmtEpAdeKp6xMtPjAPsL+RQ1xTzX83V2jeNib9ZPM6xT1DdAEz2yt4+q4JfYcQk4HWlkgUEE9Fmf22dze3FeS85gkphdC1JTH/QcPT+f7D353TqvzsUILku+//95lj7zm8rB852WjBYtZJyQhcG3Y1zVAE1Dk+DzxxBNu39wTcr9193xIqesWzyTnaxDnLorPzfMPdTsQNKDpMIJZT+eyL+kL+RwGHCDcBLAD8UaAk9psjmIyK86gchQe/jiZ0AYZbzuIitGcDBVWUOkKN0OclMhKM6M4fB83NJxsuLjMZpWIdr2N4BAJmk3f0FwUzbqwLuRaJAekDeVdOLFx4uDmihMfD03cELAfkhIqBeFGjYqGW7du1YsLxwlN67BfneuRJBRucJ999pkeWwQFeDtEgIA3AFRiQ7a4mZWHhx0qVqHiotmsEtnrydGNOJr0oakTtgHnJc5dHAucD6gAmFId/+BcxwMZlWPRPBQPSuxLEypZodtwPNjRDt+bTm3wRo7yZDT3Qs4Eynmxr3F8zNwuZBvjfEB9CGRrYzvwHTSrRDEAzl9/nEOoYI0Ka6iX0LlzZ811QC4bHgBovmn2G+DNfSW5YBuQE4rgHg8NBMQPP/yw19/HfQzNgNE1Oe5zuN6xr3Cu4Thiv6KYxlNxA+5VuGdgXhwzBFwICNDUD3WgUDSFeh+43tDsEH2AICfKzNVJCORo4X5t36wSEPj4835rPh/wHMBzAec0zvmMLnIZk+O6RY43rhM0f8d5j2sQxx/90Jj9vGA9KCbDdpQrV073O9KIAA2VWJF+BHm+pi8eI8i7hnZu5oImkdWqVTOyZcumzclKly6tzYTsu/NEc67XX3/dyJ07tzY3s98NaBLz5ptvGgUKFDDSpk2rTVXQDMW+qQrExMRoM5ccOXJoE65mzZoZ+/bt02XZN63z1F3r8ePHtUkQthXNZp577jnj5MmTbptmOi/DXXNHV/vJFXTBOmTIEG12g7QWKlTIGDBggEMTM0/rccXdurEM5+Z5Z86cMV566SUjV65cRrp06bR5m33zKvtmVzgGzpz3kyfr16/XpkWZM2fWtKD538cff+wwz4oVK4yaNWvqeYMmeE2aNDF2797tMI8/joW7bm4PHjyoTXhxPqC5GM7jhQsXOsxjNs+aPXt2vO/7ej7geNg3FXTVBPDatWtGmzZtdJvwN1dNLJ988kn928aNGw1vzJkzx3jiiSe0GSuOe+HChY2uXbsap06dcpgPTVy7d+9uFCxYUOdD01Wk8fz58347h8z9jqaCaNaI6wDre+qpp3Q7fbmv+NKs0ttrxBU0iX300Ud1O7Bs8/7n7tw076vOzejmzp1r/N///Z+eM/ggTTgvcR+zgiaN2Ge4h6KpJJr54rtmk0HcQ3r37m3kz59ftxPX1aZNmzTt9s1ufWlWieWjCS3uyVhn5cqVHfarp33gy/0Whg0bpucBmo/a77siTs+bxF63rtLvDPu0b9++2rTXvH/h508//TTevNu3bzeeffZZbUaLfYTtbdmypbFy5Uqv0mclAv8kOgQihYpcKNNFD2qeOq4hCiVo+og3SF/q8BBR8Am5OgzJBVnWzpANiqwhqx4WiUIFsqLRZwfqMhBRaAu5OgzJBWVhKDdFWT8GSzGbh6FMD21liUIZymhRVwDlpigDRZfdRBTaGDAkECryoBkf2hijVio65EDlOFQkIQp1qOSLilU471HJCx1+EVFoYx0GIiIissQ6DERERGSJAQMRERGFRh0GdP2KUbrQuUhSjH1AREQUqgzD0A6g0NFXYjqTCoqAAcECWx4QERElHLrTRq+RIR0wmF28IrHo4pKIiIi8gyHJ8dKd2C73gyJgMIshECz4JWC4c0dk3bp7P9eqhZFJEr9MIiKiAJbYIn2fCjNGjBihA9AgSsFoWRi0xNXQns4wKEnp0qUlffr0OsjL4sWLJUXduCGCwZXwwc9ERETkv4ABnbV069ZNfvrpJ+20CKN+YWhRDOfpzsaNG6V169Y6AuD27ds1yMDHajx1IiIiCpGOmzDGNnIaEEi4Gz+hVatWGlAsXLjQNg3DQWMoWwxT6235C4YZxpjgfimSQICTKdO9nzF2uDfDehIREQUhfz1DE1WHASsHjAHuDsaV79Wrl8M0jGk+f/58t9+Ji4vTj31iiYjCydGjR+X8+fMS6HLlyqVdhFPoS5OYvhF69uwpNWvWlPLly7ud7/Tp05I3b16Hafgd0z3VlRgyZEhCN42IKOiDhVKly8iN2OsS6NJHZZB9e/cwaAgDCQ4YUJcB9RDWr1/v3y0SkQEDBjjkSphNQoiIwgFyFhAs5Hyqt6TNGbj3vlsXjsmFhaN1e5nLEPoSFDB0795d6ySsXbvWshMIjGJ35swZh2n43dPodpGRkfohIgpnCBYi8xVP6c0g8r2VBOpHIlj47rvvZNWqVRIdHW35nerVq8vKlSsdpqGFBaanmLRpRT744N4HPxMREZH/chhQDDFt2jRZsGCB9sVg1kNA7cuoqCj9uV27dlKwYEGthwA9evSQ2rVry+jRo6Vx48YyY8YM2bJli3zxxReSYtKlE+nbN+XWT0REFMo5DJ999pm2jKhTp47kz5/f9pk5c6ZDZZ1Tp07Zfq9Ro4YGGQgQKlasKHPmzNEWEp4qShIREVEQ5zB402XDmjVr4k177rnn9BMw0DX0tm33fq5ShV1DExERhcJYEn6H7qCrVbv3MztuIiIispTwgbGJiIgobDBgICIiIksMGIiIiMgSAwYiIiKyxICBiIiILDFgICIiIkvh2awS3UEPGvTPz0RERORReAYM6Bp68OCU3goiIqKgwSIJIiIishSeOQx374rs2XPv5zJlRFIxbiIiIvIkPAOG2FgRc/Ardg1NRERkia/WREREZIkBAxEREVliwEBERESWGDAQERGRJQYMREREZIkBAxEREVkKz2aV6A66T59/fiYiIiKPwrdr6FGjUnoriIiIggaLJIiIiMhS+HYNffTovZ8LF2bX0ERERBbCt2vo6Oh7P7NraCIiIksskiAiIiJLDBiIiIjIEgMGIiIi8n/AsHbtWmnSpIkUKFBAIiIiZP78+R7nX7Nmjc7n/Dl9+rSvqyYiIqJgCRhiYmKkYsWKMn78eJ++t2/fPjl16pTtkydPHl9XTURERMHSSqJRo0b68RUChGzZsvn8PSIiIgqjZpWVKlWSuLg4KV++vAwePFhq1qzpdl7Mh4/pypUr/t2YNGlEXnvtn5+JiIjIoyR/WubPn18mTJggDz74oAYBEydOlDp16sjPP/8sVapUcfmdESNGyJAhQ5JuoyIjRXwsUiEiIgpnSR4wlCpVSj+mGjVqyMGDB2Xs2LEydepUl98ZMGCA9OrVyyGHoVChQkm9qURERORGiuTHV6tWTdavX+/275GRkfpJMoYhcv78vZ9z5RKJiEi6dREREYWAFAkYduzYoUUVKeb6ddTCvPczu4YmIiLyf8Bw7do1OXDggO33w4cPawCQI0cOKVy4sBYnnDhxQqZMmaJ/HzdunERHR0u5cuXkxo0bWodh1apVsmzZMl9XTURERMESMGzZskUee+wx2+9mXYP27dvL5MmTtY+Fo+ZIkCJy8+ZN6d27twYRGTJkkAoVKsiKFSsclkFEREQhFjCghYOBOgBuIGiw169fP/0QERFR8OJYEkRERGSJAQMRERFZYsBARERElsKzX2R0B92+/T8/ExERkUfh+bREp1BOlTOJiIjIPRZJEBERkaXwzGFAs1D09ggZMrBraCIiIgvhmcOAYCFTpnsfM3AgIiIit8IzYCAiIiKfMGAgIiIiSwwYiIiIyBIDBiIiIrLEgIGIiIgsMWAgIiIiS+HZD0Pq1CItWvzzMxEREXkUngFD+vQis2en9FYQEREFDRZJEBERkSUGDERERGQpPAOGmJh740fgg5+JiIjIo/AMGIiIiMgnDBiIiIjIEgMGIiIissSAgYiIiCwxYCAiIiJLDBiIiIjI/wHD2rVrpUmTJlKgQAGJiIiQ+fPnW35nzZo1UqVKFYmMjJTixYvL5MmTJUWhO+gnn7z3YdfQRERE/g8YYmJipGLFijJ+/Hiv5j98+LA0btxYHnvsMdmxY4f07NlTOnXqJEuXLpUU7Rp60aJ7H/xMRERE/h1LolGjRvrx1oQJEyQ6OlpGjx6tv5cpU0bWr18vY8eOlQYNGvi6eiIiIgrFwac2bdok9erVc5iGQAE5De7ExcXpx3TlypUk2bajR4/K+fPnJdDlypVLChcunNKbQUQUtILlfh/I9/wkDxhOnz4tefPmdZiG3xEExMbGSlRUVLzvjBgxQoYMGZJ0GxUTI3dz55acsbFSRkSuS2BLH5VB9u3dE5AnEBFRMAQLpUqXkRuxgX63D+x7fkAObz1gwADp1auX7XcEF4UKFfLrOlLFxkpGEcnZ8A3JkreYBKpbF47JhYWjNTIOtJOHiCgY4P6JYCHnU70lbU7/PkvC6Z6f5AFDvnz55MyZMw7T8HuWLFlc5i4AWlPgkxzS5CgoqfIVT5Z1ERFRykGwEMn7feD2w1C9enVZuXKlw7Tly5frdCIiIgrRgOHatWvaPBIfs9kkfkYZkVmc0K5dO9v8r7zyihw6dEj69esne/fulU8//VRmzZolb775pj/TQURERIEUMGzZskUqV66sH0BdA/w8cOBA/f3UqVO24AHQpHLRokWaq4D+G9C8cuLEiWxSSUREFER8rsNQp04dMQzD7d9d9eKI72zfvt33rSMiIqKAEJ5jSaRKJVerVpU1ImJERKT01hAREQW88AwYoqJk/xdfyGMiciNNupTeGiIiooAXngEDERER+YQBAxEREVkKz4AhJkYeqFtXzqJ04tY/Y1YQERGRa6nCtsevy5cld0pvBBERUZAI24CBiIiIvMeAgYiIiCwxYCAiIiJLDBiIiIjIEgMGIiIishSeAUOqVBJTtqxsZtfQREREXgnPgCEqSvZNnSrV2DU0ERGRV8IzYCAiIiKfMGAgIiIiS+EZMFy/LuWeekoOo3Ti9s2U3hoiIqKAF54Bg2FI5KlTUvTvn4mIiMiz8AwYiIiIyCcMGIiIiMgSAwYiIiKyxICBiIiILDFgICIiIkvhGTBEREhssWKy6++fiYiIyLPwDBgyZJA9s2dLeRGJTZMupbeGiIgo4IVnwEBEREQ+YcBARERESRMwjB8/XooWLSrp06eXhx9+WH755Re3806ePFkiIiIcPvheirp+Xco895z8zq6hiYiIkiZgmDlzpvTq1UsGDRok27Ztk4oVK0qDBg3k7Nmzbr+TJUsWOXXqlO3z559/SooyDIk6dEjK/f0zERER+TlgGDNmjHTu3FleeuklKVu2rEyYMEEyZMggX331ldvvIFchX758tk/evHl9XS0REREFS8Bw8+ZN2bp1q9SrV++fBaRKpb9v2rTJ7feuXbsmRYoUkUKFCknTpk1l1y5t0OhWXFycXLlyxeFDREREQRIwnD9/Xu7cuRMvhwC/nz592uV3SpUqpbkPCxYskG+++Ubu3r0rNWrUkOPHj7tdz4gRIyRr1qy2DwINIiIiCuFWEtWrV5d27dpJpUqVpHbt2jJv3jzJnTu3fP75526/M2DAAPnrr79sn2PHjiX1ZhIREZEHacQHuXLlktSpU8uZM2ccpuN31E3wRtq0aaVy5cpy4MABt/NERkbqh4iIiIIwhyFdunRStWpVWblypW0aihjwO3ISvIEijd9++03y588vKSYiQuLy55cjf/9MREREfsxhADSpbN++vTz44INSrVo1GTdunMTExGirCUDxQ8GCBbUeAgwdOlQeeeQRKV68uFy+fFlGjRqlzSo7deokKSZDBtm1cKEGP/nSpBPmZZCnAPfWrVvcQeQXyKFNkyaNthwjCvmAoVWrVnLu3DkZOHCgVnRE3YQlS5bYKkIePXpUW06YLl26pM0wMW/27Nn1Ib1x40ZtkkkUyNC6B5VzDfbVQX6EZujIYUWOLVFIBwzQvXt3/biyZs0ah9/Hjh2rH6Jgy1lAsICbOyrp8o2QEguBJ5qm44Xr8OHDUqJECYeXK6KQDBiCXmyslHrxRUGH1i1v3xT29UjOUAyBGzyChaioKO4g8gucS6j4jWJZBA8p3k0+kQ/CM7y9e1cy7t4tD6HOI7ObyQPmLJC/MVeBglV4BgxERETkEwYMREREZIkBAxElO1SORnEPmlrD5MmTJVu2bCl+JOrUqSM9e/ZM6c0gCkgMGIhCDLpS79ixoxQoUECb7mHgtx49esiFCxcC5iGM8WQw1D3GiiGi4MCAgSiEHDp0SDtV279/v0yfPl27YMcQ9GZvrBcvXpRAgEAG3cmzUilR8AjbgOFWtmxyLqU3goJPTIz7z40b3s8bG+vdvD7q1q2bPoyXLVumg70VLlxYGjVqJCtWrJATJ07I22+/bZsXD+v58+c7fB/FAigeMPXv319Kliyp/VEUK1ZM3n33XYeeLwcPHqydt02dOlWKFi2qOQbPP/+8XL16Vf/eoUMH+fHHH+Wjjz7S9eFz5MiReEUSrmCE2ypVqmjTQ6x7yJAhcvv2bZfzIr2Yz3l5yFl5/PHH9WfksLRu3Vp7okV6HnjgAQ2qPPFmHyFHp2XLljo9R44c0rRpU00jUagJz4AhY0b5beVKyYMuGdKyY2jyQaZM7j/NmzvOmyeP+3kbNXKct2hR1/P5ALkHS5culddeey1e3xF4m2/btq3MnDnTp54rM2fOrA/H3bt360P/yy+/jNcR28GDB/WhunDhQv0gQBg5cqT+Dd9BzgZ6e0URBD7eDFe/bt067WYeD3ysG6PbYjvee+89l/PXrVtXH9hz58516HwL6UW64caNG9rT7KJFi+T333+XLl26yIvoj+UX9MiSMAieGjRooPsJ27xhwwbJlCmTNGzYUPtZIAol4RkwEIUgFEMgGChTpozLv2M6umpHT4Peeuedd7S+AXIPmjRpIn369JFZs2Y5zIMB6PAwL1++vNSqVUsfwuYAdchxQI4H3ugRtOCD8RSsIDfhrbfe0nFrkLtQv359GTZsmAYOrmCZyNmYNm2abRq2ATkOzf8O5JCzgO1HjgiW+frrr+uD3Tk9vkBAgvRPnDhRcyywjydNmqRd5Dv3eksU7MKzp0eihLp2zf3fnB+EZ8+6n9e5S2A/ZmFb5SD4MoYBHoj/+c9/NBcBY2ugSCBLliwO8yCYwBu2CeMknPWUdi/s3LlT39btcxSQY4BcguvXr2sA4gw5CRjo7uTJk1rh89tvv5XGjRvbWl/g+8OHD9cAAcUzyAGIi4tzuSxfthP1ROzTD9hO7DOiUBKeAUNsrJTo0kVWi8hL7BqafJExY8rP6wZGhEWZ+549e+SZZ56J93dMR1fX5gMU8zoHF/b1EzZt2qQPYbztI9sduQUzZsyQ0aNHO3wHXR3bw3Lx1p0YCE6w3meffTbe39x1p/zQQw/J/fffr9v46quvynfffedQ1wAj5aKIBCPsIjcgY8aM2nrDU9GB1T7CdqKYA8GJM+xrolASngHD3buSeetWqfN319AcS4JCQc6cOTXr/tNPP5U333zToR4DRovFQw2VIu0faKhTYF+kgbd3E0aVRZNM+4qSGAPBV8jRwNu9L1DZcd++fRoE+QIBDtJ53333aRfMyGEwIccCFRJfeOEF/R1BzR9//OFx5FyrfYTtRC5Mnjx54uW8EIUa1mEgCiGffPKJZrMjR2Dt2rVagx/DzyOQQGsHDEtvQusBzL99+3bZsmWLvPLKKw65BRhNEWXxeGNH9jqKJvDW7isUWfz888/acuD8+fNe5T5gO6dMmaK5DLt27dLcEWwH6lRYBQzbtm3ToowWLVpIZGSkQ3qWL1+ugRCW17VrVzlz5ozH5VntI6wvV65cGoig0iNGoUTdhTfeeENHOyUKJQwYiEIIHoqbN2/WSn1o6occAjSrRLBg1uA3oWgBLRZQUbFNmzZaIdC+PP/pp5/WnAoMZY+KgnjQolmlr7BcVErEmzze2BGEWEHAgxYXaC6JogbUTUDrDKTHE+RIVKtWTX799Vdb6wgTgg3kCGDZ6EwKFTCbNWvmcXlW+wg/IzBD81UUn6DS48svv6x1GJjjQKEmwvCljVUKuXLlipaf/vXXX/65CNG+/e8bZ7E278vdQuUkUMWdPiCnv+4pW7du1ZsdJQ/c8PG2GB0dHfRDEA8aNEjGjBmjb9d48FLgn1vIJUHdiHztx0lkPt+KZZJTsNyfgmV/JtU+9dczNDzrMBCFEWTro1jgp59+0rdvDq9MRAnBgIEoDLz00kspvQlEFOTCtg7DnfTpxfeOd4mIiMJTeAYMGTPKTlQAY9fQREREXgnPgIHIS0FQJ5iCDM8pClYMGIhcMMc74ABC5G9mx0/OPWQSBbrwrPR444bc/8YbshDDAd/5p5tXIlOaNGm0jT0GasKNnS0LyB85CwgWMM4Guuf2ZhAuokASngHDnTuSdcMGQaexr9+9K4nr9Z5CEcYQwCBKaC+fkO6QidxBsIBOo4iCTXgGDERejoGAnhNZLEH+gtwq5ixQsGLAQOQBiiKCvadHIqIUq/Q4fvx47TkON9KHH35YfvnlF4/zz549W0qXLq3zY1jZxYsXJ3R7iYiIKBgCBgzl2qtXL+2fHv1zV6xYUQdzQUUeVzBgTevWrXVAFoz4hsFe8Pn999/9sf1EREQUiAEDBrHp3LmzdjWL0ecmTJigtcm/+uorl/N/9NFH0rBhQ+nbt6+O5DZs2DAdUANDxhIREVEI1mFA5S+MoDVgwACHMt569erJpk2bXH4H05EjYQ85EvPnz3e7nri4OP2YMMKWOeKWX2C0yr/dPHtIbgVw5zy3Lh7X/7Hfr127JoEM58Ldu8HR5iRYtpXbGZ77c9++fbaRC+/evCGBKljuT8GyP+33Kfanv5555nIS3WmY4YMTJ05gbcbGjRsdpvft29eoVq2ay++kTZvWmDZtmsO08ePHG3ny5HG7nkGDBul6+OE+4DnAc4DnAM8BngPil31w7NgxIzECspUEcjDscyXwRnDx4kXJmTOnto/3JaoqVKiQHDt2LFFjgAejcE17uKY7nNMerukO57SHa7oTmnbkLFy9elUKFCggieFTwJArVy5tQ3zmzBmH6fjdXUckmO7L/BAZGakf585OEgo7NdxOqnBPe7imO5zTHq7pDue0h2u6E5L2rFmzSrJWekRHNlWrVpWVK1c6vP3j9+rVq7v8Dqbbzw/Lly93Oz8REREFHp+LJFBU0L59e3nwwQelWrVqMm7cOImJidFWE9CuXTspWLCgjBgxQn/v0aOH1K5dW0aPHi2NGzeWGTNmyJYtW+SLL77wf2qIiIgoMAKGVq1a6YA8AwcOlNOnT0ulSpVkyZIlkjdvXv370aNHHQbqqVGjhkybNk3eeecd+de//qVd7aKFRPny5SWpoVgD/UU4F2+Eg3BNe7imO5zTHq7pDue0h2u6UzrtEaj5mOxrJSIiotDvGpqIiIjCCwMGIiIissSAgYiIiCwxYCAiIqLQCxj8PbQ26nyixUf+/PklKipKx8XYv3+/hHq6O3TooL1m2n8wSFgg8iXtu3btkubNm+v8SBOa/SZ2maGS7sGDB8c75jhHApEvaf/yyy+lVq1akj17dv3gGnaePxSvc2/SHarX+bx587RpPzr0y5gxo7bWmzp1asgf83lepDtJj7kRRGbMmGGkS5fO+Oqrr4xdu3YZnTt3NrJly2acOXPG5fwbNmwwUqdObXzwwQfG7t27jXfeeUfHtvjtt99s84wcOdLImjWrMX/+fGPnzp3G008/bURHRxuxsbFGKKe7ffv2RsOGDY1Tp07ZPhcvXjQCja9p/+WXX4w+ffoY06dPN/Lly2eMHTs20csMlXRjjJZy5co5HPNz584ZgcbXtLdp00bHp9m+fbuxZ88eo0OHDnpNHz9+PKSvc2/SHarX+erVq4158+bp/e3AgQPGuHHj9J63ZMmSkD7mq71Id1Ie86AKGDDAVbdu3Wy/37lzxyhQoIAxYsQIl/O3bNnSaNy4scO0hx9+2Ojatav+fPfuXb25jho1yvb3y5cvG5GRkXrjDdV0mydV06ZNjUDna9rtFSlSxOWDMzHLDOZ0I2CoWLGiEegSe3xu375tZM6c2fj6669D+jq3Sne4XOemypUr68tROB1z53Qn9TEPmiIJc2htZCv5MrS2/fzm0Nrm/IcPH9bOp+znQX/byBZyt8xQSLdpzZo1kidPHilVqpS8+uqrcuHCBQkkCUl7SizT35JyG5EliwFoihUrJm3bttWO1gKJP9J+/fp1uXXrluTIkSOkr3OrdIfLdY4XXww/gCGsH3300bA55oaLdCf1MQ+agOH8+fNy584dW4+SJvyOE8MVTPc0v/m/L8sMhXQDyrSmTJmiJ9z7778vP/74ozRq1EjXFSgSkvaUWKa/JdU24mY5efJk7Zn1s88+05sqysAxil2g8Efa+/fvr0GReSMO1evcKt2hfp3/9ddfkilTJh3jCMMOfPzxx1K/fv2QP+Z/eUh3Uh/zgBzempLe888/b/sZlSIrVKgg999/v0amdevW5SEIQbhpmHC8EUAUKVJEZs2aJS+//LKEgpEjR+p4NTiPUYksXLhLdyhf55kzZ5YdO3bItWvX9OGIcY6Qc1anTh0JZZkt0p2UxzxochiSYmht839fh99OTsk1pDhOOKzrwIEDEigSkvaUWKa/Jdc2oqZ1yZIlQ+aYf/jhh/rgXLZsmd4kTaF6nVulO9Svc2TfFy9eXFsK9O7dW1q0aGEb9DCUj3kqD+lO6mMeNAFDUgytHR0drQfGfp4rV67Izz//HDDDbyfXkOLHjx/Xci40QQoUCUl7SizT35JrG/GGcvDgwZA45h988IEMGzZMi1vQ7MxeqF7nVukOt+sc34mLiwv5Y+4p3Ul+zI0ggiYoqOU6efJkbVbSpUsXbYJy+vRp/fuLL75ovPXWWw7NC9OkSWN8+OGH2uwItcRdNavEMhYsWGD8+uuvWrs0EJve+DPdV69e1SZ4mzZtMg4fPmysWLHCqFKlilGiRAnjxo0bRiDxNe1xcXHazAyf/Pnzazrx8/79+71eZqimu3fv3saaNWv0mOMcqVevnpErVy7j7NmzRiDxNe24htE0bc6cOQ5NyXCeh/J1bpXuUL7Ohw8fbixbtsw4ePCgzo97He55X375ZUgf8+EW6U7qYx5UAQN8/PHHRuHChfVCQZOUn376yfa32rVra5MSe7NmzTJKliyp86MN+qJFixz+juY37777rpE3b149cHXr1jX27dtnhHK6r1+/bjzxxBNG7ty5NZBAMzy0/w2kB2ZC046LBHGw8wfzebvMUE13q1atNJjA8goWLKi/oy13IPIl7Th/XaUdgXIoX+dW6Q7l6/ztt982ihcvbqRPn97Inj27Ub16dX342gvFY/62RbqT+phzeGsiIiIKnToMRERElHIYMBAREZElBgxERERkiQEDERERWWLAQERERJYYMBAREZElBgxERERkiQEDERERWWLAQEQOBg8erEPsRkREyPz585N17xw5ckTXi9H4iCiwMGAgcqFDhw764MIogPbwAMX0ULVnzx4ZMmSIfP7553Lq1CmHIbGTQ6FChXS95cuX198xJC/29+XLl5N1O4goPgYMRG6kT59e3n//fbl06VLY7COMXglNmzbV0f4iIyMTtJxbt24l6HsY7hfrTZMmjQSzmzdvpvQmEPkdAwYiN+rVq6cPL09jzcPcuXOlXLly+nAtWrSojB492uHvmDZ8+HDp2LGjZM6cWQoXLixffPGF5X7ftWuXPPXUU5IlSxb9Xq1atWwPdAxpO3ToULnvvvt0vZUqVdIhjp2z9mfNmqXfi4qKkoceekj++OMP2bx5sw6FnClTJs1BOHfunK0ookmTJvduDKlS2XJSvF3XzJkzpXbt2hpoffvtt5pL06xZM007ijiyZcumy7l9+7b07dtXcuTIocucNGmSyyIJ/PzYY4/p9OzZs+t0LHPKlCmSM2fOeEP6Yl0vvvii2wd49+7ddYhfbF+RIkUcjityMLp27arbib8jh2PhwoU+HWMMM92uXTs9Xl26dNHp69evt+1/5J688cYbEhMTY3nsiQKSX4awIgoxGCEOw+HOmzdPR4Y7duyYTv/uu+90REDTli1bjFSpUhlDhw7VkfAmTZpkREVF6f8mjBiXI0cOY/z48Trc9IgRI/Q7e/fudbv+48eP63eeffZZY/Pmzbrsr776yvadMWPGGFmyZDGmT5+u0/r166ej0/3xxx8Oo1eWLl3aWLJkiQ6F+8gjjxhVq1Y16tSpY6xfv97Ytm2bjnz3yiuv2IbGxXbje+ZQyb6sq2jRosbcuXONQ4cOGSdPntR9mDlzZqNbt276vf/+9786X4MGDYz33ntPvz9s2DBdlrl/zWVhaO7bt2/r8vA70o/tuXz5so7IlzVrVh2R1XTmzBkd5nfVqlUu9+eoUaOMQoUKGWvXrjWOHDlirFu3zpg2bZr+7c6dO7pvMKqrOXTw999/byxevNinY4x9hOGGMQKo+cmYMaMxduxYTSuGFK9cubLRoUMHn89HokDAgIHIQ8AAeJh07NjRZcDQpk0bo379+g7f7du3r1G2bFmHh8kLL7zgMOxunjx5jM8++8ztvh8wYIARHR1t3Lx50+XfCxQooA9dew899JDx2muvOTx4J06caPs7HviYtnLlSts0BC+lSpWy/e6cPl/WNW7cuHj7EGnHA9mEddWqVcv2O4ICPFSxbc4BA6xevVp/v3TpksOyX331VaNRo0a230ePHm0UK1ZM960rr7/+uvH444+7/PvSpUs1IHA39LG3x7hZs2YO87z88stGly5dHKYhUMG6YmNjXa6LKJCxSILIAuoxfP3111oh0Bmm1axZ02Eaft+/f7/cuXPHNq1ChQq2n5G1jqKOs2fP6u8oFkDxAD7I9gZkySMrO23atPHWeeXKFTl58qTL9Tpvo/16kd0ODzzwgMM0cztc8WVdKOZwhvSgeMN+ffbrR50FFC942gZXOnfuLMuWLZMTJ07o75MnT7ZVVHUFf8M+LVWqlBYL4LsmTEfRSMmSJV1+19tj7Jz+nTt36naZxxafBg0aaBHP4cOHfUovUSAI7ppFRMng0Ucf1Rv9gAED9MGTEM4PfjzY8OCAiRMnSmxsrMN8KPP2B/v1mg9T52nmdiRWxowZPa7fXJ+nfeGtypUrS8WKFbU+wxNPPKH1PRYtWuR2/ipVquhD+ocffpAVK1ZIy5YttY7KnDlz/LavndN/7do1rReBAMUZ6rEQBRsGDEReQPNKVPbDG6q9MmXKyIYNGxym4Xe8reLt2RsFCxaMNw05A8jVQGsD5wcsKtUVKFBA14NKhvbrrVatml+PZ3Kuy5V06dLp//Zv8qZOnTrJuHHjNJcBD39UKrRKS6tWrfTTokULadiwoVy8eFH39fHjx7VCqKtchoQeYwQpu3fvluLFi/uQYqLAxSIJIi8gG71t27byn//8x2F67969ZeXKlVpDHg8cPOQ/+eQT6dOnT6L2K2r0ozjg+eefly1btmj299SpU2Xfvn36d7QyQFEJWiZg2ltvvaVZ6z169PD78UzOdTlDawbkQKDFAlpz4K3d1KZNG33Qf/nll9oCxZMxY8bI9OnTZe/evXqcZs+ercVCaLmBQAi5SM2bN5fly5fbciLMliAJPcb9+/eXjRs36rHE/sIxXLBggf5OFIwYMBB5CU0CnbPO8RaJposzZszQpngDBw7U+RJadGFCuf6qVav0AYkHWtWqVfXBaOY2IJu7V69e+jBDMIOH2//+9z8pUaKE349ncq7LVe4LOpJCkIL6D/YP26xZs+pDHnUD0KTSEzRL/eCDD7SeAZqXosnm4sWLbfUr0GwS01u3bi1ly5aVfv362XI1EnqMkXPx448/apCB+igoRsF3kWNDFIwiUPMxpTeCiCgh6tatqxUrnXN+iMj/GDAQUdBB75voNhp1EVBPwLluCRH5Hys9ElHQQfY+ggbUrWCwQJQ8mMNARERElljpkYiIiCwxYCAiIiJLDBiIiIjIEgMGIiIissSAgYiIiCwxYCAiIiJLDBiIiIjIEgMGIiIiEiv/D/FsFZXbpWYTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good predictor should have low non-comformity scores, concentrated at the left side of the figure\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot histogram and quantile\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.hist(sorted_scores, edgecolor='k', linewidth=1)\n",
    "plt.axvline(\n",
    "    x=threshold, linestyle='--', color='r', label='Quantile value'\n",
    ")\n",
    "plt.title('Histogram of non-comformity scores in the calibration set')\n",
    "plt.xlabel('Non-comformity score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('A good predictor should have low non-comformity scores, concentrated at the left side of the figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47595423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : calc scores , plot dist , find threshold\n",
    "sorted_scores = np.sort(score_per_group.values)\n",
    "# threshold = sorted_scores[k - 1]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe734f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '013781df-4391-4533-bcb1-15f6819064f6',\n",
       " 132: 'ffc01d9d-bf28-4787-a42c-b92b44db3285',\n",
       " 134: 'ffc01d9d-bf28-4787-a42c-b92b44db3285',\n",
       " 137: 'ffc01d9d-bf28-4787-a42c-b92b44db3285',\n",
       " 145: 'ffc01d9d-bf28-4787-a42c-b92b44db3285',\n",
       " 178: '5cf2503b-9026-423a-882c-d19018f86fb2',\n",
       " 273: '86f52e05-c352-4bab-a101-df2ddf566b57',\n",
       " 280: '88b53da2-6476-4cef-9946-ce2904187f43',\n",
       " 282: '88b53da2-6476-4cef-9946-ce2904187f43',\n",
       " 291: '502a9d95-af93-4f9c-a017-7dcd36c64a5d',\n",
       " 295: '63be6edd-ebf5-446f-a61e-7b9990c850ae',\n",
       " 297: '63be6edd-ebf5-446f-a61e-7b9990c850ae',\n",
       " 299: '79f72a57-2967-44b5-b0cd-2501ef64769b',\n",
       " 331: '375bd12d-d44b-4453-b6e3-c0f1889ceb82',\n",
       " 333: '375bd12d-d44b-4453-b6e3-c0f1889ceb82',\n",
       " 356: '9aec1bbb-e7ac-45e4-bcbd-ec2cee25af1a',\n",
       " 420: 'a4bfaf1d-ba19-4af0-9618-8ab467eb7b5a',\n",
       " 429: 'd5602fc7-7ab7-441b-b9ca-03b64ae81261',\n",
       " 431: 'd5602fc7-7ab7-441b-b9ca-03b64ae81261',\n",
       " 437: 'de69733d-58cb-4118-a38f-ef684fc4d07d',\n",
       " 439: 'de69733d-58cb-4118-a38f-ef684fc4d07d',\n",
       " 510: 'e719ca92-3b81-4f40-b2d1-a3bf89e07c04',\n",
       " 515: 'e719ca92-3b81-4f40-b2d1-a3bf89e07c04',\n",
       " 525: '659e07c8-c514-4767-9ff0-828790fbda2d',\n",
       " 526: '659e07c8-c514-4767-9ff0-828790fbda2d',\n",
       " 587: '7462a685-56a2-4e12-a7b8-055a03655978',\n",
       " 588: '7462a685-56a2-4e12-a7b8-055a03655978',\n",
       " 593: 'c72c2a37-1eb4-415d-ac97-801f4ba59219',\n",
       " 596: 'c72c2a37-1eb4-415d-ac97-801f4ba59219',\n",
       " 604: 'eaa378ef-394c-46a3-b153-960b099f9f91',\n",
       " 636: '6f79871b-a798-4439-8415-34379a131074',\n",
       " 755: 'ad47b9a6-f8e0-4136-85b9-1d1b761d8433',\n",
       " 761: 'c7bf7015-e355-4c7d-9321-d75d37da94bc',\n",
       " 770: '2f096936-2d36-40bc-8fbd-8f91785410ea',\n",
       " 890: '5b5d29e1-547a-46da-8a35-6c7886695d9d',\n",
       " 897: '6bd5466e-3bff-4f6a-b94a-ef4bd1b079e2',\n",
       " 900: '6bd5466e-3bff-4f6a-b94a-ef4bd1b079e2',\n",
       " 999: '924476f8-37f9-42e2-bedf-64eeac0ce8b6',\n",
       " 1011: '8a1cda91-fc7a-419e-bbc1-9c7363ea15c7'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from mind2web.dataloader import MultiChoiceDataset, get_data_split\n",
    "from mind2web.evauate import FastActionEvaluatorMultiChoice  # note the file name\n",
    "# if installed as a package, you can also do: from mind2web.evauate import FastActionEvaluatorMultiChoice\n",
    "\n",
    "# Load tokenizer/model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Prepare data (adjust paths/splits as needed)\n",
    "data_dir = \"DATA_PATH\"\n",
    "split_file = \"data/test_task/*.json\"  # example split\n",
    "data = get_data_split(data_dir, split_file)\n",
    "dataset = MultiChoiceDataset(data, tokenizer, num_candidates=5, max_context_len=512, mode=\"multichoice\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = FastActionEvaluatorMultiChoice(tokenizer, max_context_len=512)\n",
    "result = evaluator.evaluate_dataset(\n",
    "    dataset,\n",
    "    model,\n",
    "    batch_size=8,\n",
    "    top_k=50,\n",
    "    output_path=None,  # set a folder path to save JSON outputs\n",
    "    name=\"test_task\",\n",
    "    template=None,     # optionally a tuple (prefix_context, suffix_input)\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca260a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 158/1339 [48:58<6:06:01, 18.60s/it, action_f1=0, element_acc=0]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m test_key = \u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m output_path = os.getcwd()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m result = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#lm_template,\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[38;5;66;43;03m#cfg.top_k,)\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/project/Robots-That-Ask-For-Help-Over-The-Horizon/Mind2Web/src/action_prediction/metric.py:189\u001b[39m, in \u001b[36mActionEvaluatorMultiChoice.evaluate_dataset\u001b[39m\u001b[34m(self, dataset, model, batch_size, top_k, output_path, name, template)\u001b[39m\n\u001b[32m    177\u001b[39m device = \u001b[38;5;28mnext\u001b[39m(model.parameters()).device\n\u001b[32m    178\u001b[39m model_input = {\n\u001b[32m    179\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: torch.LongTensor(model_input[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    180\u001b[39m     .unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m     .to(device),\n\u001b[32m    187\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m decoded_output = \u001b[38;5;28mself\u001b[39m.tokenizer.batch_decode(\n\u001b[32m    195\u001b[39m     output, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    196\u001b[39m )\n\u001b[32m    197\u001b[39m outputs[-\u001b[32m1\u001b[39m][-\u001b[32m1\u001b[39m] = decoded_output[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:1764\u001b[39m, in \u001b[36mT5ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1761\u001b[39m         decoder_attention_mask = decoder_attention_mask.to(\u001b[38;5;28mself\u001b[39m.decoder.first_device)\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1764\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m sequence_output = decoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:1100\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m   1098\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1118\u001b[39m \u001b[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:711\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    709\u001b[39m do_cross_attention = \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     cross_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m     hidden_states = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    724\u001b[39m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:640\u001b[39m, in \u001b[36mT5LayerCrossAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, query_length, output_attentions, cache_position)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    627\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    637\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    638\u001b[39m ):\n\u001b[32m    639\u001b[39m     normed_hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m     layer_output = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(attention_output[\u001b[32m0\u001b[39m])\n\u001b[32m    653\u001b[39m     outputs = (layer_output,) + attention_output[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:514\u001b[39m, in \u001b[36mT5Attention.forward\u001b[39m\u001b[34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m     key_states = \u001b[38;5;28mself\u001b[39m.k(current_states)\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     value_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     key_states = key_states.view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.key_value_proj_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    516\u001b[39m     value_states = value_states.view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.key_value_proj_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/BaryGNN/anaconda3/envs/conf/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('./Mind2Web/src') \n",
    "\n",
    "# Force reload the metric module\n",
    "if 'action_prediction.metric' in sys.modules:\n",
    "    importlib.reload(sys.modules['action_prediction.metric'])\n",
    "\n",
    "from action_prediction.metric import ActionEvaluatorGeneration, ActionEvaluatorMultiChoice\n",
    "\n",
    "evaluator = ActionEvaluatorMultiChoice(tokenizer)\n",
    "test_dataset = dataset\n",
    "test_key = \"task\"\n",
    "output_path = os.getcwd()\n",
    "result = evaluator.evaluate_dataset(\n",
    "    test_dataset,\n",
    "    model,\n",
    "    output_path=output_path,\n",
    "    name=test_key,\n",
    "    template=None, #lm_template,\n",
    "    top_k=50#cfg.top_k,)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(test_task_ds)\n",
    "# len(df.groupby(\"annotation_id\"))\n",
    "# subsample[0]['action_reprs']\n",
    "\n",
    "# test_domain_ds = ds['test_domain']\n",
    "# test_task_ds = ds['test_task']\n",
    "# test_website_ds = ds['test_website']\n",
    "count = 0\n",
    "missing_count = 0\n",
    "mulit = {}\n",
    "missing = {}\n",
    "for i, x in enumerate(test_task_ds):\n",
    "    if len(x['pos_candidates']) > 1:\n",
    "        count += 1\n",
    "        mulit[i] = x['annotation_id']\n",
    "    if len(x['pos_candidates']) == 0:\n",
    "        # print('Missing pos_candidates:', x['annotation_id'])\n",
    "        missing_count += 1\n",
    "        missing[i] = x['annotation_id']\n",
    "\n",
    "print(\"Number of samples with multiple positive candidates in test_website:\", count)\n",
    "print(\"Number of samples with missing positive candidates in test_website:\", missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2edcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import json\n",
    "import pathlib\n",
    "import sys\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Import the repository's dataset utilities\n",
    "from action_prediction.dataloader import MultiChoiceDataset, get_data_split  # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-name\", default=\"google/flan-t5-base\")\n",
    "    parser.add_argument(\"--data-dir\", required=True, help=\"data dir or dataset script used by get_data_split\")\n",
    "    parser.add_argument(\"--split-file\", required=True, help=\"path to split json file or list accepted by get_data_split\")\n",
    "    parser.add_argument(\"--num-samples\", type=int, default=10)\n",
    "    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser.add_argument(\"--out\", default=\"multichoice_generations.json\")\n",
    "    parser.add_argument(\"--max-new-tokens\", type=int, default=50)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    outputs = generate_with_dataset(\n",
    "        model_name=args.model_name,\n",
    "        data_dir=args.data_dir,\n",
    "        split_file=args.split_file,\n",
    "        num_samples=args.num_samples,\n",
    "        device=args.device,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "    )\n",
    "\n",
    "    with open(args.out, \"w\") as f:\n",
    "        json.dump(outputs, f, indent=2)\n",
    "    print(f\"Wrote {len(outputs)} generations to {args.out}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73519215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: rent a car in Brooklyn - Central, NY on from April 9 to April 15.\n",
      "step=1/7 | op={\"original_op\": \"CLICK\", \"value\": \"\", \"op\": \"CLICK\"} | target_action=[heading]  CAR -> CLICK | pos_candidates=1, action_id=0\n",
      "step=2/7 | op={\"original_op\": \"TYPE\", \"value\": \"Brooklyn Central\", \"op\": \"TYPE\"} | target_action=[combobox]  Enter pick up city, airport name, or airport code. -> TYPE: Brooklyn Central | pos_candidates=1, action_id=1\n",
      "step=3/7 | op={\"original_op\": \"CLICK\", \"value\": \"\", \"op\": \"CLICK\"} | target_action=[div]  Brooklyn - Central (New York), US -> CLICK | pos_candidates=1, action_id=2\n",
      "step=4/7 | op={\"original_op\": \"CLICK\", \"value\": \"\", \"op\": \"CLICK\"} | target_action=[textbox]  Pickup -> CLICK | pos_candidates=1, action_id=3\n",
      "step=5/7 | op={\"original_op\": \"CLICK\", \"value\": \"\", \"op\": \"CLICK\"} | target_action=[button]  Sunday, April 9, 2023 -> CLICK | pos_candidates=1, action_id=4\n",
      "step=6/7 | op={\"original_op\": \"CLICK\", \"value\": \"\", \"op\": \"CLICK\"} | target_action=[button]  Saturday, April 15, 2023 -> CLICK | pos_candidates=1, action_id=5\n",
      "step=7/7 | op={\"original_op\": \"CLICK\", \"value\": \"\", \"op\": \"CLICK\"} | target_action=[button]  Find cars button. -> CLICK | pos_candidates=1, action_id=6\n"
     ]
    }
   ],
   "source": [
    "# Group by annotation_id (this creates a GroupBy object for fast access)\n",
    "grouped = df.groupby('annotation_id')\n",
    "\n",
    "# Retrieve all rows for a specific annotation_id\n",
    "ann_id = train_ds[0][\"annotation_id\"]\n",
    "task_df = grouped.get_group(ann_id).sort_values('target_action_index')\n",
    "\n",
    "print(f\"Task: {task_df.iloc[0]['confirmed_task']}\")\n",
    "# Iterate and display (task_df is a DataFrame)\n",
    "for _, ex in task_df.iterrows():\n",
    "    print(\n",
    "        f\"step={int(ex['target_action_index']) + 1}/{len(task_df)} | op={ex['operation']} \"\n",
    "        f\"| target_action={ex[\"target_action_reprs\"]} | pos_candidates={len(ex['pos_candidates'])}, action_id={ex['action_id']}\"\n",
    "    )\n",
    "    # display(train_ds[ex[\"action_id\"]][\"screenshot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d5e8173",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m./Mind2Web/src\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcandidate_generation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CandidateRankDataset, get_data_split\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcandidate_generation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CERerankingEvaluator\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcandidate_generation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/yandex/MLWG2025/amitr5/project/Robots-That-Ask-For-Help-Over-The-Horizon/Mind2Web/src/candidate_generation/metric.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxml\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_candidate\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlxml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m etree\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dataloader'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add Mind2Web to path so we can import from it\n",
    "sys.path.append('./Mind2Web/src')\n",
    "\n",
    "from candidate_generation.dataloader import CandidateRankDataset, get_data_split\n",
    "from candidate_generation.metric import CERerankingEvaluator\n",
    "from candidate_generation.model import CrossEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eval_data = train_ds\n",
    "batch_size = 350\n",
    "max_seq_length = 512\n",
    "\n",
    "eval_evaluator = CERerankingEvaluator(\n",
    "    eval_data,\n",
    "    k=50,\n",
    "    max_neg=-1,\n",
    "    batch_size=batch_size,\n",
    "    name=\"train\",\n",
    ")\n",
    "\n",
    "# Use the model path for the CrossEncoder (like in evaluate.py)\n",
    "model_path = \"osunlp/MindAct_CandidateGeneration_deberta-v3-base\"\n",
    "model = CrossEncoder(\n",
    "    model_path,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    num_labels=1,\n",
    "    max_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Running evaluation...\")\n",
    "eval_evaluator(model, output_path=\"./output\")\n",
    "print(\"Evaluation completed!\")\n",
    "print(\"Results saved to: ./output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465dad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Download the MindAct Candidate Generation model\n",
    "print(\"Downloading MindAct Candidate Generation model...\")\n",
    "model_name = \"osunlp/MindAct_CandidateGeneration_deberta-v3-base\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load model\n",
    "candidate_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "    device_map=\"auto\"  # Automatically handle device placement\n",
    ")\n",
    "\n",
    "print(f\"Model {model_name} downloaded and loaded successfully!\")\n",
    "print(f\"Model type: {type(candidate_model)}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in candidate_model.parameters()):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
